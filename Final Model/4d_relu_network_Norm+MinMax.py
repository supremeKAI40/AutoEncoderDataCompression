# -*- coding: utf-8 -*-
"""AE 4d network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EFkyZ_U_UMOzLfnQCYlT5Q59DduOb7Lv
"""

#We need to update fastAI in google collab
pip install fastai --upgrade

#to check version, 2.2.7 seems to work best.
#updating also changes pytorch versions
import fastai
fastai.__version__

"""#Reading CSV

Here the CSV files have events seperated by semi colons and each jets are in a cell separated by comma.
"""

from csv import reader
import csv

list=[]
with open('/content/monojet_Zp2000.0_DM_50.0_chan3.csv', 'r') as read_obj:
    # pass the file object to reader() to get the reader object
    csv_reader = reader(read_obj, delimiter=';', skipinitialspace=True, quotechar=',')
    # Iterate over each row in the csv using reader object
    for row in csv_reader:
        # row variable is a list that represents a row in csv
        print(row)
        list.append(row)

#checking number of lines in the dataset which are each triggers
len(list)

"""Upon perusal of CSV data, each particle trigger is specifically 5 entries, the id of object and 4 momenta

Using this pattern to extract all the object detections from the given CSV file without event id or string name because both are same for our dataset.
"""

objects=[]
for i in range(len(list)):
    for rows in list[i]:
        test=rows.split(',')
        if(len(test)==5):
            objects.append(test)

len(objects)

"""#EDA and Data Visualisation"""

import pandas as pd
from pandas import DataFrame

df = DataFrame(objects)
#Assigning Column names as given in data discription
column_names=['obj1', 'E1', 'pt1', 'eta1', 'phi1']
df.columns= column_names

df.head(6)

"""As per the task provided we have to compress data only for jets which are depicted using symbol j.

So, I will extract those rows where objects are jets.
"""

data_j= df.loc[df['obj1'] == 'j'].reset_index(drop=True)
print(data_j)

data_j.tail()

import numpy as np

#converting string type in dataframe to flaot to work with tensors.
data_j['E1']=data_j['E1'].astype(float)
data_j['pt1']=data_j['pt1'].astype(float)
data_j['eta1']=data_j['eta1'].astype(float)
data_j['phi1']=data_j['phi1'].astype(float)

data_j.describe()

"""Let's make a copy of the original data to work with different process of Analysis"""

dataset=data_j.copy()

#obj1 column is same for all
dataset=dataset.drop(['obj1'], axis=1)

features=dataset.columns

"""##Plotting and visualising data"""

import matplotlib.pyplot as plt
variable_list = ['E', r'$p_T$', r'$\eta$', r'$\phi$']
branches=["E1","pt1","eta1","phi1"]

n_bins = 100

for kk in range(0,4):
    hist_data= plt.hist(dataset[branches[kk]], bins=n_bins)
    plt.xlabel(variable_list[kk])
    plt.ylabel('No. of Occurances')
    plt.show()
plt.close()

"""#Data Preparation"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import torchvision.transforms as tt #to tansform image into tensors
from torch.utils.data import random_split #for spliting dataset into train and validation set
from torchvision.utils import make_grid 
import datetime
import time

#to draw images in notebook
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

#making a test dataset from the given part
train, test = train_test_split(dataset, test_size=0.1, random_state=42)

print(train.shape, test.shape)

"""##Normalisation of Data

As visible from previous plots, different values of columns have very distributed range of values and we might need Normalisation to account for outliers and unbiased training.
"""

#Trying Standard Normalisation
train_mean = train.mean()
train_std= train.std()
print(train_mean, train_std)
std_normalized_train = (train - train_mean) / train_std
std_normalized_test = (test - train_mean) / train_std

print(std_normalized_train.mean(),std_normalized_test.mean())

'''Using Custom Normalisation parameters drived from GSoc 2020 open source project 
master's thesis of Eric Wulff'''

#Although data may be different, the manual normalisation still works better than StandardScalar or Std Normalisation
E1_div = 2.5
E1_add = 1
pt1_sub = 1.3
pt1_div = 1.2
eta1_div = 5
phi1_div = 3

#For Custom Normalisation
def custom_normalization(train, test):
    train_cp = train.copy()
    test_cp = test.copy()
    
    for data in [train_cp, test_cp]:
        data['pt1'] = (np.log10(data['pt1']) - pt1_sub) / pt1_div
        data['phi1'] = data['phi1'] / phi1_div
        data['eta1'] = data['eta1'] / eta1_div
        data['E1'] = np.log10(data['E1'] + E1_add) / E1_div
    return train_cp, test_cp

#To Denormalise and Visualise Physical data
def custom_unnormalize(normalized_data):
    data = normalized_data.copy()

    data['pt1'] = np.power(10, pt_div * data['pt1'] + pt1_sub)
    data['eta1'] = data['eta1'] * eta1_div
    data['phi1'] = data['phi1'] * phi1_div
    data['E1'] =  np.power(10, E1_div * data['E1']) - E1_add

    return data

custom_normalized_train, custom_normalized_test = custom_normalization(train, test)
unnormalized_test = custom_unnormalize(custom_normalized_test)

(np.abs(test - unnormalized_test) < 1e-10).all()

custom_normalized_train.describe()

from sklearn.preprocessing import StandardScaler, MinMaxScaler
minmaxOncustom_train= custom_normalized_train.copy()
minmaxOncustom_test= custom_normalized_test.copy()

features=custom_normalized_train.columns
autoscaler = MinMaxScaler()
minmaxOncustom_train[features] = autoscaler.fit_transform(minmaxOncustom_train[features])
minmaxOncustom_test[features] = autoscaler.fit_transform(minmaxOncustom_test[features])

minmaxOncustom_train.describe()

"""###Plot of Custom Normalisation Parameters"""

variable_list = ['E', r'$p_T$', r'$\eta$', r'$\phi$']
branches=["E1","pt1","eta1","phi1"]

n_bins = 100

for kk in range(0,4):
    hist_data= plt.hist(custom_normalized_train[branches[kk]], bins=n_bins)
    plt.xlabel(variable_list[kk])
    plt.ylabel('No. of Occurances')
    plt.show()
plt.close()

"""###Plot of Standard Normalisation """

variable_list = ['E', r'$p_T$', r'$\eta$', r'$\phi$']
branches=["E1","pt1","eta1","phi1"]

n_bins = 100

for kk in range(0,4):
    hist_data= plt.hist(std_normalized_train[branches[kk]], bins=n_bins)
    plt.ylabel('No. of Occurances')
    plt.show()
plt.close()

"""###Plot of MinMaxScalar over Custom Normalisation"""

variable_list = ['E', r'$p_T$', r'$\eta$', r'$\phi$']
branches=["E1","pt1","eta1","phi1"]

n_bins = 100

for kk in range(0,4):
    hist_data= plt.hist(minmaxOncustom_train[branches[kk]], bins=n_bins)
    plt.xlabel(variable_list[kk])
    plt.ylabel('No. of Occurances')
    plt.show()
plt.close()

#train, val = train_test_split(std_normalized_train, test_size=0.15, random_state=42)
#train1, val1 = train_test_split(custom_normalized_train, test_size=0.15, random_state=42)
train1, val1 = train_test_split(minmaxOncustom_train, test_size=0.15, random_state=42)

#train_x= std_normalized_train
train_x = train1
val_x = val1
train_y = train_x  # y = x since we are building an autoencoder
val_y = val_x
#test_x= std_normalized_test
#test_x= custom_normalized_test
test_x= minmaxOncustom_test
test_y=test_x

# Constructs a tensor object of the data and wraps them in a TensorDataset object.
train_ds = TensorDataset(torch.tensor(train_x.values, dtype=torch.float), torch.tensor(train_y.values, dtype=torch.float))
valid_ds = TensorDataset(torch.tensor(val_x.values, dtype=torch.float), torch.tensor(val_y.values, dtype=torch.float))
test_ds = TensorDataset(torch.tensor(test_x.values, dtype=torch.float), torch.tensor(test_y.values, dtype=torch.float))

from fastai.data import core

bs = 256

# Converts the TensorDataset into a DataLoader object and combines into one DataLoaders object (a basic wrapper
# around several DataLoader objects). 
train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)
valid_dl = DataLoader(valid_ds, batch_size=bs * 2)
dls = core.DataLoaders(train_dl, valid_dl)

class AE_3D_200_LeakyReLU(nn.Module):
    def __init__(self, n_features=4):
        super(AE_3D_200_LeakyReLU, self).__init__()
        self.en1 = nn.Linear(n_features, 200)
        self.en2 = nn.Linear(200, 200)
        self.en3 = nn.Linear(200, 20)
        self.en4 = nn.Linear(20, 3)
        self.de1 = nn.Linear(3, 20)
        self.de2 = nn.Linear(20, 200)
        self.de3 = nn.Linear(200, 200)
        self.de4 = nn.Linear(200, n_features)
        self.tanh = nn.Tanh()

    def encode(self, x):
        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))

    def decode(self, x):
        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))

    def forward(self, x):
        z = self.encode(x)
        return self.decode(z)

    def describe(self):
        return 'in-200-200-20-3-20-200-200-out'

#model = AE_3D_200_LeakyReLU().double()
model = AE_3D_200_LeakyReLU()
model.to('cpu')

from fastai import learner

from fastai.metrics import mse

loss_func = nn.MSELoss()

#bn_wd = False  # Don't use weight decay for batchnorm layers
#true_wd = True  # weight decay will be used for all optimizers
wd = 1e-6

recorder = learner.Recorder()
learn = learner.Learner(dls, model=model, wd=wd, loss_func=loss_func, cbs=recorder)

from fastai.callback import schedule

lr_min, lr_steep = learn.lr_find()

print('Learning rate with the minimum loss:', lr_min)
print('Learning rate with the steepest gradient:', lr_steep)

import time

start = time.perf_counter() # Starts timer
learn.fit_one_cycle(n_epoch=100, lr_max=lr_min)
end = time.perf_counter() # Ends timer
delta_t = end - start
print('Training took', delta_t, 'seconds')

recorder.plot_loss()

learn.validate()

import os
save_dir = "plotOutput"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

test_y

import numpy as np

plt.close('all')
unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']
variable_list = [r'$p_T$', r'$\eta$', r'$\phi$', r'$E$']
line_style = ['--', '-']
colors = ['orange', 'c']
markers = ['*', 's']

model.to('cpu')

# Histograms
data_X = torch.tensor(test_x.values, dtype=torch.float)
pred = model(data_X)
pred = pred.detach().numpy()
residuals = np.true_divide(pred - test_x.values,test_x.values)
pred_df = pd.DataFrame(pred, columns=dataset.columns)    

variable_list = ['E', r'$p_T$', r'$\eta$', r'$\phi$']
branches=["E1","pt1","eta1","phi1"]

alph = 0.8
n_bins = 200

for kk in range(0,4):
    plt.figure()
    hist_data,bin_edges,_= plt.hist(test_y[branches[kk]], color=colors[1], label='Input', alpha=1, bins=n_bins)
    n_hist_pred,_,_= plt.hist(pred_df[branches[kk]], color=colors[0], label='Output', alpha=alph, bins=bin_edges)
    plt.suptitle(dataset.columns[kk])
    plt.ylabel('No. of Occurances')
    plt.yscale('log')
    plt.savefig(os.path.join(save_dir,test.columns[kk]+'.png'))
    plt.legend()
plt.close()

residuals

def getRatio(bin1,bin2):
    bins = []
    for b1,b2 in zip(bin1,bin2):
        if b1==0 and b2==0:
            bins.append(0.)
        elif b2==0:
            bins.append(None)
        else:
            bins.append((float(b2)-float(b1))/b1)
    return bins   

rat = getRatio(hist_data,n_hist_pred)
print(np.mean(rat))

hist_data,bin_edges,_= plt.hist(rat, color=colors[1], alpha=1, bins=n_bins)

torch.save(model.state_dict(), '/content/customNormScaleModel')