# -*- coding: utf-8 -*-
"""AE 4d network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EFkyZ_U_UMOzLfnQCYlT5Q59DduOb7Lv
"""

pip install fastai --upgrade

from csv import reader
import csv

list=[]
with open('/content/monojet_Zp2000.0_DM_50.0_chan3.csv', 'r') as read_obj:
    # pass the file object to reader() to get the reader object
    csv_reader = reader(read_obj, delimiter=';', skipinitialspace=True, quotechar=',')
    # Iterate over each row in the csv using reader object
    for row in csv_reader:
        # row variable is a list that represents a row in csv
        print(row)
        list.append(row)

len(list)

data=[]
for i in range(len(list)):
    for rows in list[i]:
        test=rows.split(',')
        if(len(test)==5):
            data.append(test)

len(data)

import pandas as pd
from pandas import DataFrame

df = DataFrame(data)
column_names=['obj1', 'E1', 'pt1', 'eta1', 'phi1']
df.columns= column_names

df.head(6)

data_j= df.loc[df['obj1'] == 'j'].reset_index(drop=True)
print(data_j)

data_j.tail()

import numpy as np

data_j['E1']=data_j['E1'].astype(float)
data_j['pt1']=data_j['pt1'].astype(float)
data_j['eta1']=data_j['eta1'].astype(float)
data_j['phi1']=data_j['phi1'].astype(float)

data_j.describe()

data_j.columns

from sklearn.preprocessing import StandardScaler, MinMaxScaler
dataset=data_j.copy()

dataset=dataset.drop(['obj1'], axis=1)

features=dataset.columns
autoscaler = MinMaxScaler()
dataset[features] = autoscaler.fit_transform(dataset[features])

dataset['E1'].max()

dataset.describe()

len(dataset)

import matplotlib.pyplot as plt
variable_list = ['E', r'$p_T$', r'$\eta$', r'$\phi$']
branches=["E1","pt1","eta1","phi1"]

n_bins = 100

for kk in range(0,4):
    hist_data= plt.hist(dataset[branches[kk]], bins=n_bins)
    plt.ylabel('No. of Occurances')
    plt.show()
plt.close()

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import torchvision.transforms as tt #to tansform image into tensors
from torch.utils.data import random_split #for spliting dataset into train and validation set
from torchvision.utils import make_grid 
import datetime
import time

#to draw images in notebook
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

train, val = train_test_split(dataset, test_size=0.2, random_state=42)
n_features = len(train.loc[0])
train, test = train_test_split(dataset, test_size=0.1, random_state=42)

print(train.shape, test.shape)
print(val.shape)

train_x = train
val_x = val
train_y = train_x  # y = x since we are building an autoencoder
val_y = val_x
test_x= test
test_y=test_x

# Constructs a tensor object of the data and wraps them in a TensorDataset object.
train_ds = TensorDataset(torch.tensor(train_x.values, dtype=torch.float), torch.tensor(train_y.values, dtype=torch.float))
valid_ds = TensorDataset(torch.tensor(val_x.values, dtype=torch.float), torch.tensor(val_y.values, dtype=torch.float))
test_ds = TensorDataset(torch.tensor(test_x.values, dtype=torch.float), torch.tensor(test_y.values, dtype=torch.float))

from fastai.data import core

bs = 256

# Converts the TensorDataset into a DataLoader object and combines into one DataLoaders object (a basic wrapper
# around several DataLoader objects). 
train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)
valid_dl = DataLoader(valid_ds, batch_size=bs * 2)
dls = core.DataLoaders(train_dl, valid_dl)

class AE_3D_200_LeakyReLU(nn.Module):
    def __init__(self, n_features=4):
        super(AE_3D_200_LeakyReLU, self).__init__()
        self.en1 = nn.Linear(n_features, 200)
        self.en2 = nn.Linear(200, 200)
        self.en3 = nn.Linear(200, 20)
        self.en4 = nn.Linear(20, 3)
        self.de1 = nn.Linear(3, 20)
        self.de2 = nn.Linear(20, 200)
        self.de3 = nn.Linear(200, 200)
        self.de4 = nn.Linear(200, n_features)
        self.tanh = nn.Tanh()

    def encode(self, x):
        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))

    def decode(self, x):
        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))

    def forward(self, x):
        z = self.encode(x)
        return self.decode(z)

    def describe(self):
        return 'in-200-200-20-3-20-200-200-out'

#model = AE_3D_200_LeakyReLU().double()
model = AE_3D_200_LeakyReLU()
model.to('cpu')

from fastai import learner

from fastai.metrics import mse

loss_func = nn.MSELoss()

#bn_wd = False  # Don't use weight decay for batchnorm layers
#true_wd = True  # weight decay will be used for all optimizers
wd = 1e-6

recorder = learner.Recorder()
learn = learner.Learner(dls, model=model, wd=wd, loss_func=loss_func, cbs=recorder)

from fastai.callback import schedule

lr_min, lr_steep = learn.lr_find()

print('Learning rate with the minimum loss:', lr_min)
print('Learning rate with the steepest gradient:', lr_steep)

import time

start = time.perf_counter() # Starts timer
learn.fit_one_cycle(n_epoch=100, lr_max=lr_min)
end = time.perf_counter() # Ends timer
delta_t = end - start
print('Training took', delta_t, 'seconds')

recorder.plot_loss()

learn.validate()

import os
save_dir = "plotOutput"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

test_y

import numpy as np

plt.close('all')
unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']
variable_list = [r'$p_T$', r'$\eta$', r'$\phi$', r'$E$']
line_style = ['--', '-']
colors = ['orange', 'c']
markers = ['*', 's']

model.to('cpu')

save = False # Option to save figure

# Histograms
#data = torch.tensor(test[idxs[0]:idxs[1]].values, dtype=torch.float).double()
data_X = torch.tensor(test_x.values, dtype=torch.float)
pred = model(data_X)
pred = pred.detach().numpy()

pred_df = pd.DataFrame(pred, columns=dataset.columns)    

variable_list = ['E', r'$p_T$', r'$\eta$', r'$\phi$']
branches=["E1","pt1","eta1","phi1"]

alph = 0.8
n_bins = 200

for kk in range(0,4):
    plt.figure()
    hist_data,bin_edges,_= plt.hist(test_y[branches[kk]], color=colors[1], label='Input', alpha=1, bins=n_bins)
    n_hist_pred,_,_= plt.hist(pred_df[branches[kk]], color=colors[0], label='Output', alpha=alph, bins=bin_edges)
    plt.suptitle(dataset.columns[kk])
    plt.ylabel('No. of Occurances')
    plt.yscale('log')
    if save:
        plt.savefig(os.path.join(save_dir,test.columns[kk]+'.png'))
    plt.legend()
plt.close()

def getRatio(bin1,bin2):
    bins = []
    for b1,b2 in zip(bin1,bin2):
        if b1==0 and b2==0:
            bins.append(0.)
        elif b2==0:
            bins.append(None)
        else:
            bins.append((float(b2)-float(b1))/b1)
    return bins   

rat = getRatio(hist_data,n_hist_pred)
print(np.median(rat), np.mean(rat))

torch.save(model.state_dict(), '/content/MinMaxScaleModel')